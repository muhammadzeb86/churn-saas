# üöÄ **PHASE 2: PRODUCTION HARDENING - DETAILED IMPLEMENTATION**

**Date:** December 13, 2025  
**For Review:** User + DeepSeek Cross-Check  
**Goal:** Highway-Grade Production Code  
**Timeline:** 32 hours over 4 days

---

## üìã **EXECUTIVE SUMMARY**

### **Current State**
- ‚úÖ Phase 1 Complete: ML pipeline working end-to-end
- ‚úÖ Explanation column fixed and Excel-friendly
- ‚úÖ 149 tests passing (~85% coverage)
- ‚úÖ Infrastructure automated (Terraform)
- ‚ö†Ô∏è Database not optimized (no indexes)
- ‚ö†Ô∏è No comprehensive test coverage
- ‚ö†Ô∏è No API documentation
- ‚ö†Ô∏è Error messages too technical

### **Phase 2 Objectives**
1. Achieve 95%+ test coverage (200+ tests)
2. Optimize database queries (<50ms)
3. Add comprehensive security hardening
4. Create Swagger/OpenAPI documentation
5. Implement user-friendly error handling
6. Achieve <100ms API response times

### **Why Phase 2 Before Phase 4?**
- **Prevents rework:** Solid backend before building dashboard
- **Reduces bugs:** Comprehensive testing catches issues early
- **Improves performance:** Database optimization before scale
- **Enhances security:** Audit before customer data exposure
- **Enables confidence:** Well-documented API for frontend team

---

## üéØ **TASK 2.1: COMPREHENSIVE TESTING (8 HOURS)**

### **Goal**
Increase from 149 tests (85% coverage) to 200+ tests (95%+ coverage)

### **Strategy**

**Current Test Structure:**
```
backend/tests/
‚îú‚îÄ‚îÄ test_column_mapper.py      (25 tests) ‚úÖ
‚îú‚îÄ‚îÄ test_feature_validator.py  (20 tests) ‚úÖ
‚îú‚îÄ‚îÄ test_simple_explainer.py   (16 tests) ‚úÖ
‚îú‚îÄ‚îÄ test_monitoring.py         (25 tests) ‚úÖ
‚îú‚îÄ‚îÄ test_auth_verification.py  (30 tests) ‚úÖ
‚îî‚îÄ‚îÄ test_*.py                  (33 more tests) ‚úÖ
Total: 149 tests
```

**New Test Structure:**
```
backend/tests/
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ test_end_to_end_pipeline.py       (15 tests) üÜï
‚îÇ   ‚îú‚îÄ‚îÄ test_saas_baseline_integration.py (10 tests) üÜï
‚îÇ   ‚îî‚îÄ‚îÄ test_error_recovery.py            (12 tests) üÜï
‚îú‚îÄ‚îÄ performance/
‚îÇ   ‚îú‚îÄ‚îÄ test_large_csvs.py                (8 tests) üÜï
‚îÇ   ‚îî‚îÄ‚îÄ test_concurrent_predictions.py    (6 tests) üÜï
‚îú‚îÄ‚îÄ security/
‚îÇ   ‚îú‚îÄ‚îÄ test_jwt_edge_cases.py            (10 tests) üÜï
‚îÇ   ‚îú‚îÄ‚îÄ test_input_validation.py          (15 tests) üÜï
‚îÇ   ‚îî‚îÄ‚îÄ test_rate_limiting.py             (8 tests) üÜï
‚îî‚îÄ‚îÄ test_edge_cases.py                    (10 tests) üÜï
Total New: 94 tests ‚Üí Grand Total: 243 tests
```

### **Implementation Details**

#### **2.1.1: Integration Tests (3 hours)**

**File: `tests/integration/test_end_to_end_pipeline.py`**

```python
"""
End-to-end integration tests for the full prediction pipeline.
Tests: Upload ‚Üí SQS ‚Üí Worker ‚Üí Prediction ‚Üí S3 ‚Üí Download

Purpose: Verify all components work together in production-like conditions
Strategy: Use real AWS services (with test credentials) or mocked equivalents
"""
import pytest
import asyncio
from httpx import AsyncClient
from backend.main import app
from backend.models import Prediction, Upload, PredictionStatus
from backend.api.database import get_async_session
import boto3
from moto import mock_s3, mock_sqs
import pandas as pd
import io

# ========================================
# FIXTURES
# ========================================

@pytest.fixture
async def test_client():
    """Create test HTTP client"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        yield client

@pytest.fixture
def mock_aws_services():
    """Mock AWS S3 and SQS services for testing"""
    with mock_s3(), mock_sqs():
        # Create test S3 bucket
        s3 = boto3.client('s3', region_name='us-east-1')
        s3.create_bucket(Bucket='retainwise-uploads-test')
        
        # Create test SQS queue
        sqs = boto3.client('sqs', region_name='us-east-1')
        queue = sqs.create_queue(
            QueueName='retainwise-prediction-queue-test.fifo',
            Attributes={'FifoQueue': 'true'}
        )
        
        yield {
            's3': s3,
            'sqs': sqs,
            'queue_url': queue['QueueUrl']
        }

@pytest.fixture
def sample_csv_data():
    """Generate sample CSV data for testing"""
    data = {
        'customerID': [f'CUST_{i:03d}' for i in range(1, 11)],
        'tenure': [12, 6, 24, 3, 36, 18, 48, 9, 15, 27],
        'MonthlyCharges': [99.0, 49.0, 149.0, 29.0, 199.0, 79.0, 249.0, 59.0, 89.0, 129.0],
        'TotalCharges': [1188, 294, 3576, 87, 7164, 1422, 11952, 531, 1335, 3483],
        'Contract': ['Annual', 'Monthly', 'Annual', 'Monthly', 'Annual', 
                     'Quarterly', 'Annual', 'Quarterly', 'Annual', 'Annual']
    }
    df = pd.DataFrame(data)
    return df.to_csv(index=False)

# ========================================
# TEST CASES
# ========================================

class TestEndToEndPipeline:
    """Test the complete prediction pipeline from upload to download"""
    
    @pytest.mark.asyncio
    async def test_upload_csv_creates_prediction(self, test_client, sample_csv_data, mock_aws_services):
        """
        Test 1: CSV Upload ‚Üí Prediction Record Created
        
        Logic: When user uploads CSV, system should:
        1. Validate CSV format
        2. Upload to S3
        3. Create prediction record in DB (status=QUEUED)
        4. Publish message to SQS
        
        Why this test: Verifies first stage of pipeline works
        """
        # Create CSV file in memory
        csv_file = io.BytesIO(sample_csv_data.encode())
        csv_file.name = "test_customers.csv"
        
        # Upload CSV
        response = await test_client.post(
            "/upload",
            files={"file": ("test_customers.csv", csv_file, "text/csv")},
            headers={"Authorization": "Bearer test_token"}
        )
        
        # Assertions
        assert response.status_code == 200
        data = response.json()
        assert 'upload_id' in data
        assert 'prediction_id' in data
        assert data['status'] == 'queued'
        
        # Verify DB record created
        async with get_async_session() as session:
            prediction = await session.get(Prediction, data['prediction_id'])
            assert prediction is not None
            assert prediction.status == PredictionStatus.QUEUED
            
        # Verify S3 upload
        s3 = mock_aws_services['s3']
        objects = s3.list_objects_v2(Bucket='retainwise-uploads-test')
        assert objects['KeyCount'] > 0
        
        # Verify SQS message published
        sqs = mock_aws_services['sqs']
        messages = sqs.receive_message(
            QueueUrl=mock_aws_services['queue_url'],
            MaxNumberOfMessages=1
        )
        assert 'Messages' in messages
        assert len(messages['Messages']) == 1
    
    @pytest.mark.asyncio
    async def test_worker_processes_prediction(self, mock_aws_services, sample_csv_data):
        """
        Test 2: Worker Picks Up SQS Message ‚Üí Processes Prediction
        
        Logic: Worker should:
        1. Poll SQS queue
        2. Download CSV from S3
        3. Run prediction (column mapping + ML)
        4. Upload results to S3
        5. Update DB (status=COMPLETED)
        6. Delete SQS message
        
        Why this test: Verifies worker processing logic
        """
        # TODO: Implement worker processing test
        # This requires mocking the entire worker flow
        pass
    
    @pytest.mark.asyncio
    async def test_saas_baseline_generates_explanations(self, test_client, sample_csv_data):
        """
        Test 3: SaaS Baseline Model ‚Üí Excel-Friendly Output
        
        Logic: When CSV processed with SaaS baseline:
        1. risk_level column populated (High/Medium/Low)
        2. churn_risk_pct formatted as percentage
        3. recommendation column clear and actionable
        4. key_risks comma-separated (not JSON)
        5. strengths comma-separated (not JSON)
        
        Why this test: Verifies Excel-friendly formatting fix works
        """
        # Upload and process CSV
        # ... (similar to test 1)
        
        # Download results
        response = await test_client.get(
            f"/predictions/{prediction_id}/download",
            headers={"Authorization": "Bearer test_token"}
        )
        
        assert response.status_code == 200
        
        # Parse CSV
        csv_content = response.content.decode('utf-8')
        df = pd.read_csv(io.StringIO(csv_content))
        
        # Verify Excel-friendly columns exist
        assert 'risk_level' in df.columns
        assert 'churn_risk_pct' in df.columns
        assert 'recommendation' in df.columns
        assert 'key_risks' in df.columns
        assert 'strengths' in df.columns
        
        # Verify formats
        assert df['risk_level'].iloc[0] in ['High', 'Medium', 'Low']
        assert df['churn_risk_pct'].iloc[0].endswith('%')
        assert 'RISK' in df['recommendation'].iloc[0]
        
        # Verify no raw JSON (comma-separated text instead)
        assert not df['key_risks'].iloc[0].startswith('[')
        assert not df['key_risks'].iloc[0].startswith('{')
    
    @pytest.mark.asyncio
    async def test_column_mapper_handles_variations(self, test_client):
        """
        Test 4: Column Mapper ‚Üí Handles 50+ Variations
        
        Logic: Test common column name variations:
        - customer_id, Customer ID, CUSTOMER_ID, custID, cust_id
        - tenure, Tenure, TENURE, months, contract_months
        - etc.
        
        Why this test: Verifies intelligent column mapper works
        """
        variations = [
            # Variation 1: Different case
            {'customer_id': 'CUST_001', 'Tenure': 12, 'monthly_charges': 99.0},
            # Variation 2: Spaces
            {'Customer ID': 'CUST_002', 'Contract Months': 6, 'Monthly Price': 49.0},
            # Variation 3: Abbreviated
            {'custID': 'CUST_003', 'months': 24, 'mrr': 149.0}
        ]
        
        for variant in variations:
            df = pd.DataFrame([variant])
            csv_data = df.to_csv(index=False)
            
            # Upload and verify successful mapping
            response = await test_client.post(
                "/upload",
                files={"file": ("test.csv", csv_data, "text/csv")},
                headers={"Authorization": "Bearer test_token"}
            )
            
            assert response.status_code == 200
            assert response.json()['status'] == 'queued'

# ========================================
# ERROR RECOVERY TESTS
# ========================================

class TestErrorRecovery:
    """Test system behavior under failure conditions"""
    
    @pytest.mark.asyncio
    async def test_s3_upload_failure_graceful_error(self, test_client, sample_csv_data, monkeypatch):
        """
        Test 5: S3 Upload Fails ‚Üí Graceful Error
        
        Logic: When S3 upload fails:
        1. Don't crash the API
        2. Return user-friendly error message
        3. Log error details (for debugging)
        4. Don't create prediction record
        
        Why this test: Prevents user-facing crashes
        """
        # Mock S3 to raise exception
        def mock_upload_error(*args, **kwargs):
            raise Exception("S3 connection timeout")
        
        monkeypatch.setattr("backend.services.s3_service.upload_file", mock_upload_error)
        
        # Attempt upload
        response = await test_client.post(
            "/upload",
            files={"file": ("test.csv", sample_csv_data, "text/csv")},
            headers={"Authorization": "Bearer test_token"}
        )
        
        # Verify graceful failure
        assert response.status_code == 500
        error = response.json()
        assert 'error' in error
        assert 'ERR-' in error['error']['code']  # Standard error code format
        assert 'S3' not in error['error']['message']  # User-friendly (no tech details)
    
    @pytest.mark.asyncio
    async def test_database_connection_retry(self, test_client, monkeypatch):
        """
        Test 6: Database Connection Fails ‚Üí Retry ‚Üí Success
        
        Logic: When DB connection fails transiently:
        1. Retry up to 3 times with exponential backoff
        2. If all retries fail, return error
        3. If retry succeeds, continue normally
        
        Why this test: Handles transient network issues
        """
        call_count = {'count': 0}
        
        async def mock_db_connection(*args, **kwargs):
            call_count['count'] += 1
            if call_count['count'] < 3:
                raise Exception("Connection refused")
            # Success on 3rd attempt
            return original_get_session(*args, **kwargs)
        
        # This test verifies retry logic exists
        # Actual implementation will use @retry_async decorator
        pass

# ========================================
# REASONING FOR THIS APPROACH
# ========================================

"""
Why Integration Tests?
-----------------------
1. **Catch Integration Bugs:** Unit tests don't catch issues between components
2. **Real-World Scenarios:** Test exactly what users will do
3. **Confidence:** Know the system works end-to-end before deploy
4. **Regression Prevention:** Catch when changes break existing flows

Why Mock AWS Services?
----------------------
1. **Speed:** No network latency (tests run fast)
2. **Cost:** No AWS charges for testing
3. **Isolation:** Tests don't affect production data
4. **Reliability:** Tests work even if AWS has outages

Why These Specific Tests?
--------------------------
1. **Upload ‚Üí Prediction:** Most critical user flow
2. **SaaS Baseline:** Recent fix needs verification
3. **Column Mapper:** Core value proposition
4. **Error Recovery:** Prevents user-facing crashes

Alternative Approaches Considered:
----------------------------------
1. **Real AWS in test env:** Rejected (slow, costs money, flaky)
2. **E2E tests only:** Rejected (too slow for CI/CD)
3. **No integration tests:** Rejected (risky, bugs in production)
4. **Manual testing only:** Rejected (not reproducible, slow)

Chosen Approach: Integration tests with mocked AWS
--------------------------------------------------
‚úÖ Fast (run in CI/CD)
‚úÖ Reliable (no network dependencies)
‚úÖ Comprehensive (covers critical paths)
‚úÖ Maintainable (standard pytest patterns)
"""
```

**Estimated Time:** 3 hours
- Write tests: 2 hours
- Debug and fix issues found: 1 hour

---

#### **2.1.2: Performance Tests (2 hours)**

**File: `tests/performance/test_large_csvs.py`**

```python
"""
Performance tests for large CSV processing.
Tests: 1K, 10K, 100K row CSVs

Purpose: Ensure system handles scale without degradation
Strategy: Measure processing time, memory usage, throughput
"""
import pytest
import pandas as pd
import time
import psutil
import os

# ========================================
# TEST DATA GENERATION
# ========================================

def generate_large_csv(num_rows: int) -> pd.DataFrame:
    """
    Generate synthetic customer data for performance testing
    
    Logic: Create realistic SaaS customer data at scale
    - Uses numpy for speed (10K rows in ~0.1s)
    - Realistic distributions (not all values same)
    - Mix of Annual/Monthly contracts
    - Varied usage patterns
    
    Why: Need consistent test data that mimics production
    """
    import numpy as np
    
    np.random.seed(42)  # Reproducible results
    
    data = {
        'customerID': [f'CUST_{i:06d}' for i in range(1, num_rows + 1)],
        'tenure': np.random.randint(1, 120, num_rows),
        'MonthlyCharges': np.random.uniform(10, 500, num_rows).round(2),
        'TotalCharges': np.random.uniform(10, 50000, num_rows).round(2),
        'Contract': np.random.choice(['Annual', 'Monthly', 'Quarterly'], num_rows, p=[0.5, 0.3, 0.2]),
        'seats_used': np.random.randint(1, 100, num_rows),
        'feature_usage_score': np.random.randint(0, 100, num_rows),
        'last_activity_days_ago': np.random.randint(0, 90, num_rows),
        'support_tickets': np.random.randint(0, 20, num_rows)
    }
    
    return pd.DataFrame(data)

# ========================================
# PERFORMANCE TEST CASES
# ========================================

class TestLargeCSVProcessing:
    """Test system performance with large datasets"""
    
    @pytest.mark.performance
    def test_1000_rows_under_10_seconds(self):
        """
        Test: 1,000 rows ‚Üí Complete in <10s
        
        Logic:
        1. Generate 1K row CSV
        2. Run full prediction pipeline
        3. Measure total time
        4. Verify <10s end-to-end
        
        Why: 1K rows is common for SMB SaaS companies
        Target: <10s (acceptable for user experience)
        """
        # Generate data
        df = generate_large_csv(1000)
        
        # Start timing
        start_time = time.time()
        start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB
        
        # Run prediction pipeline
        from backend.ml.saas_baseline import SaaSChurnBaseline
        model = SaaSChurnBaseline()
        predictions = model.predict(df)
        
        # End timing
        end_time = time.time()
        end_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB
        
        # Calculate metrics
        duration = end_time - start_time
        memory_used = end_memory - start_memory
        throughput = 1000 / duration  # rows/second
        
        # Assertions
        assert duration < 10.0, f"Processing took {duration:.2f}s, expected <10s"
        assert memory_used < 500, f"Used {memory_used:.2f}MB, expected <500MB"
        assert throughput > 100, f"Throughput {throughput:.0f} rows/s, expected >100"
        
        # Log metrics (for monitoring trends over time)
        print(f"\n‚úÖ 1K rows: {duration:.2f}s, {throughput:.0f} rows/s, {memory_used:.1f}MB")
    
    @pytest.mark.performance
    def test_10000_rows_under_60_seconds(self):
        """
        Test: 10,000 rows ‚Üí Complete in <60s
        
        Logic: Same as above but 10x scale
        
        Why: 10K rows is large enterprise customer base
        Target: <60s (still acceptable, user can wait)
        """
        df = generate_large_csv(10000)
        
        start_time = time.time()
        
        from backend.ml.saas_baseline import SaaSChurnBaseline
        model = SaaSChurnBaseline()
        predictions = model.predict(df)
        
        duration = time.time() - start_time
        throughput = 10000 / duration
        
        assert duration < 60.0, f"Processing took {duration:.2f}s, expected <60s"
        assert throughput > 150, f"Throughput {throughput:.0f} rows/s, expected >150"
        
        print(f"\n‚úÖ 10K rows: {duration:.2f}s, {throughput:.0f} rows/s")
    
    @pytest.mark.performance
    @pytest.mark.slow
    def test_memory_usage_stays_constant(self):
        """
        Test: Memory doesn't grow with dataset size (no leaks)
        
        Logic:
        1. Process 1K rows, measure memory
        2. Process 10K rows, measure memory
        3. Memory should scale linearly, not exponentially
        4. Memory should be released after processing
        
        Why: Memory leaks cause OOM errors in production
        Target: Linear memory growth (10x data = ~10x memory)
        """
        import gc
        
        # Baseline memory
        gc.collect()
        baseline_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
        
        # Process 1K rows
        df_1k = generate_large_csv(1000)
        from backend.ml.saas_baseline import SaaSChurnBaseline
        model = SaaSChurnBaseline()
        predictions_1k = model.predict(df_1k)
        memory_1k = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
        
        # Clear and collect garbage
        del df_1k, predictions_1k
        gc.collect()
        
        # Process 10K rows
        df_10k = generate_large_csv(10000)
        predictions_10k = model.predict(df_10k)
        memory_10k = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
        
        # Calculate memory growth
        growth_1k = memory_1k - baseline_memory
        growth_10k = memory_10k - baseline_memory
        growth_ratio = growth_10k / growth_1k
        
        # Assertions
        # Should be close to 10x (10K rows vs 1K rows)
        # Allow 20x max (some overhead is acceptable)
        assert growth_ratio < 20, f"Memory grew {growth_ratio:.1f}x (expected ~10x), possible leak"
        
        print(f"\n‚úÖ Memory: 1K={growth_1k:.1f}MB, 10K={growth_10k:.1f}MB, ratio={growth_ratio:.1f}x")

# ========================================
# REASONING
# ========================================

"""
Why Performance Tests?
----------------------
1. **Prevent Regressions:** Catch when code changes slow things down
2. **Capacity Planning:** Know system limits before hitting them
3. **User Experience:** Slow = frustrated users = churn
4. **Cost Optimization:** Faster = fewer worker hours = lower AWS bill

Why These Metrics?
------------------
1. **Processing Time:** Direct user experience metric
2. **Memory Usage:** Prevents OOM crashes
3. **Throughput:** Efficiency metric (rows/second)
4. **Memory Leaks:** Production stability

Why These Thresholds?
---------------------
1. **1K rows in 10s:** Average SMB customer count
2. **10K rows in 60s:** Large enterprise
3. **100 rows/s throughput:** Industry standard
4. **<500MB memory:** Fits in 512MB Fargate task

Alternative Approaches:
-----------------------
1. **Load testing in production:** Rejected (risky, affects real users)
2. **Manual performance testing:** Rejected (not repeatable)
3. **Only test small datasets:** Rejected (doesn't catch scale issues)
4. **No performance tests:** Rejected (blind to degradation)

Chosen Approach: Automated performance regression tests
--------------------------------------------------------
‚úÖ Runs in CI/CD on every deploy
‚úÖ Catches regressions before production
‚úÖ Documents expected performance
‚úÖ Reproducible and consistent
"""
```

**Estimated Time:** 2 hours

---

#### **2.1.3: Security Tests (2 hours)**

**File: `tests/security/test_input_validation.py`**

```python
"""
Security tests for input validation and attack prevention.
Tests: CSV injection, SQL injection, path traversal, XSS

Purpose: Ensure system is secure against common attacks
Strategy: Try malicious inputs, verify they're rejected safely
"""
import pytest
from httpx import AsyncClient
from backend.main import app
import pandas as pd
import io

# ========================================
# MALICIOUS INPUT SAMPLES
# ========================================

MALICIOUS_INPUTS = {
    'csv_injection': [
        '=1+1',              # Formula injection
        '@SUM(A1:A10)',      # Excel formula
        '+1+1',              # Addition formula
        '-1+1',              # Subtraction formula
        '=cmd|"/c calc"',    # Command injection via formula
    ],
    'sql_injection': [
        "'; DROP TABLE users; --",
        "1' OR '1'='1",
        "admin'--",
        "1' UNION SELECT * FROM users--",
    ],
    'path_traversal': [
        '../../../etc/passwd',
        '..\\..\\..\\windows\\system32',
        '%2e%2e%2f%2e%2e%2f',  # URL encoded ../../../
    ],
    'xss': [
        '<script>alert("XSS")</script>',
        '<img src=x onerror=alert(1)>',
        'javascript:alert(1)',
    ]
}

# ========================================
# TEST CASES
# ========================================

class TestCSVInjectionPrevention:
    """Test prevention of CSV formula injection attacks"""
    
    @pytest.mark.asyncio
    @pytest.mark.security
    async def test_formula_injection_neutralized(self, test_client):
        """
        Test: CSV cells starting with =+@- ‚Üí Neutralized with '
        
        Logic:
        1. Upload CSV with cells starting with dangerous characters
        2. System should prepend ' to neutralize formulas
        3. Downloaded CSV should have ' prefix
        4. Excel won't execute formulas
        
        Why: CSV injection can execute arbitrary code in Excel
        How: Prepend single quote to cells starting with =+@-
        
        Reference: https://owasp.org/www-community/attacks/CSV_Injection
        """
        # Create CSV with formula injection attempts
        malicious_data = {
            'customerID': MALICIOUS_INPUTS['csv_injection'],
            'tenure': [12] * len(MALICIOUS_INPUTS['csv_injection']),
            'MonthlyCharges': [99.0] * len(MALICIOUS_INPUTS['csv_injection']),
            'TotalCharges': [1188] * len(MALICIOUS_INPUTS['csv_injection']),
            'Contract': ['Annual'] * len(MALICIOUS_INPUTS['csv_injection'])
        }
        
        df = pd.DataFrame(malicious_data)
        csv_data = df.to_csv(index=False)
        
        # Upload
        response = await test_client.post(
            "/upload",
            files={"file": ("malicious.csv", csv_data, "text/csv")},
            headers={"Authorization": "Bearer test_token"}
        )
        
        assert response.status_code == 200
        prediction_id = response.json()['prediction_id']
        
        # Wait for processing and download
        # (In real implementation, would poll status endpoint)
        
        # Download results
        response = await test_client.get(
            f"/predictions/{prediction_id}/download",
            headers={"Authorization": "Bearer test_token"}
        )
        
        # Parse downloaded CSV
        result_df = pd.read_csv(io.StringIO(response.content.decode()))
        
        # Verify dangerous characters were neutralized
        for customer_id in result_df['customerID']:
            if customer_id.startswith(('=', '+', '@', '-')):
                # Should be prefixed with '
                assert customer_id[0] == "'", f"Formula not neutralized: {customer_id}"

class TestSQLInjectionPrevention:
    """Test prevention of SQL injection attacks"""
    
    @pytest.mark.asyncio
    @pytest.mark.security
    async def test_sql_injection_blocked(self, test_client):
        """
        Test: SQL injection in parameters ‚Üí Rejected safely
        
        Logic:
        1. Try SQL injection in various parameters
        2. System should use parameterized queries (ORM)
        3. No SQL should execute
        4. Error message shouldn't reveal DB structure
        
        Why: SQL injection can expose/delete entire database
        How: Use SQLAlchemy ORM (never raw SQL)
        
        Reference: https://owasp.org/www-community/attacks/SQL_Injection
        """
        for injection_attempt in MALICIOUS_INPUTS['sql_injection']:
            # Try injection in prediction_id parameter
            response = await test_client.get(
                f"/predictions/{injection_attempt}/download",
                headers={"Authorization": "Bearer test_token"}
            )
            
            # Should return 404 or 400, not 500 (no crash)
            assert response.status_code in [400, 404]
            
            # Error message shouldn't reveal SQL
            error_msg = response.json().get('error', {}).get('message', '')
            assert 'SELECT' not in error_msg
            assert 'DROP' not in error_msg
            assert 'TABLE' not in error_msg

class TestFileUploadSecurity:
    """Test file upload security measures"""
    
    @pytest.mark.asyncio
    @pytest.mark.security
    async def test_file_size_limit_enforced(self, test_client):
        """
        Test: Files >50MB ‚Üí Rejected
        
        Logic:
        1. Try to upload 60MB file
        2. Should be rejected before processing
        3. Clear error message about size limit
        
        Why: Large files can DoS the server
        Limit: 50MB (reasonable for customer CSVs)
        """
        # Create 60MB CSV
        large_data = {
            'customerID': [f'CUST_{i}' for i in range(500000)],  # 500K rows
            'tenure': [12] * 500000,
            'MonthlyCharges': [99.0] * 500000,
            'TotalCharges': [1188] * 500000,
            'Contract': ['Annual'] * 500000
        }
        df = pd.DataFrame(large_data)
        csv_data = df.to_csv(index=False)
        
        # Verify it's actually >50MB
        assert len(csv_data.encode()) > 50 * 1024 * 1024
        
        # Attempt upload
        response = await test_client.post(
            "/upload",
            files={"file": ("huge.csv", csv_data, "text/csv")},
            headers={"Authorization": "Bearer test_token"}
        )
        
        # Should be rejected
        assert response.status_code == 413  # Payload Too Large
        error = response.json()
        assert 'ERR-1002' in error['error']['code']
        assert '50MB' in error['error']['message']
    
    @pytest.mark.asyncio
    @pytest.mark.security
    async def test_file_type_validation(self, test_client):
        """
        Test: Non-CSV files ‚Üí Rejected
        
        Logic:
        1. Try to upload .exe, .sh, .py files
        2. Should be rejected immediately
        3. Don't execute or process
        
        Why: Prevent malicious file uploads
        How: Check file extension + content validation
        """
        malicious_files = [
            ("virus.exe", b"MZ\\x90\\x00", "application/x-msdownload"),
            ("script.sh", b"#!/bin/bash\\nrm -rf /", "application/x-sh"),
            ("hack.py", b"import os; os.system('rm -rf /')", "text/x-python")
        ]
        
        for filename, content, mimetype in malicious_files:
            response = await test_client.post(
                "/upload",
                files={"file": (filename, content, mimetype)},
                headers={"Authorization": "Bearer test_token"}
            )
            
            assert response.status_code == 400
            error = response.json()
            assert 'ERR-1001' in error['error']['code']
            assert 'CSV' in error['error']['message']
    
    @pytest.mark.asyncio
    @pytest.mark.security
    async def test_filename_sanitization(self, test_client):
        """
        Test: Malicious filenames ‚Üí Sanitized
        
        Logic:
        1. Upload CSV with path traversal in filename
        2. Filename should be sanitized before S3 upload
        3. No directory traversal possible
        
        Why: Prevent writing files outside upload directory
        How: Strip path components, keep only basename
        """
        malicious_filenames = [
            "../../../etc/passwd.csv",
            "..\\..\\..\\windows\\system32\\config.csv",
            "/etc/passwd.csv",
            "C:\\Windows\\System32\\config.csv"
        ]
        
        for filename in malicious_filenames:
            csv_data = "customerID,tenure\\nCUST_001,12"
            
            response = await test_client.post(
                "/upload",
                files={"file": (filename, csv_data, "text/csv")},
                headers={"Authorization": "Bearer test_token"}
            )
            
            if response.status_code == 200:
                # If accepted, verify filename was sanitized
                data = response.json()
                # S3 key should not contain ../ or absolute paths
                assert '../' not in data.get('s3_key', '')
                assert 'C:' not in data.get('s3_key', '')
                assert '/etc/' not in data.get('s3_key', '')

# ========================================
# REASONING
# ========================================

"""
Why Security Tests?
-------------------
1. **Prevent Attacks:** Catch vulnerabilities before hackers do
2. **Compliance:** Required for SOC2, ISO27001 certification
3. **Customer Trust:** Security breaches destroy reputation
4. **Cost Prevention:** Breaches cost millions (fines, lawsuits)

Why These Specific Tests?
--------------------------
1. **CSV Injection:** Attackers can execute code in customer's Excel
2. **SQL Injection:** #1 OWASP vulnerability, can steal all data
3. **File Upload:** Common attack vector (malware, DoS)
4. **Path Traversal:** Can read sensitive files from server

Why These Defenses?
-------------------
1. **CSV Injection:** Prepend ' to dangerous characters (OWASP recommendation)
2. **SQL Injection:** Use ORM (SQLAlchemy), never raw SQL
3. **File Size:** Limit 50MB (balance usability vs DoS risk)
4. **File Type:** Allow only CSV (reject executable files)
5. **Filename:** Sanitize before storage (prevent path traversal)

Alternative Approaches:
-----------------------
1. **No security tests:** Rejected (wait for breach?)
2. **Manual penetration testing:** Rejected (expensive, one-time)
3. **Third-party security scan:** Supplement, not replace
4. **Hope for the best:** Rejected (not highway-grade)

Chosen Approach: Automated security regression tests
-----------------------------------------------------
‚úÖ Runs on every deploy
‚úÖ Catches regressions
‚úÖ Documents security measures
‚úÖ Complements manual pen testing
"""
```

**Estimated Time:** 2 hours

---

### **Testing Summary**

**Total Tests: 94 new + 149 existing = 243 tests**

**Coverage Increase: 85% ‚Üí 95%+**

**Estimated Time: 8 hours**
- Integration tests: 3 hours
- Performance tests: 2 hours
- Security tests: 2 hours
- Edge case tests: 1 hour

---

## üóÑÔ∏è **TASK 2.2: DATABASE OPTIMIZATION (6 HOURS)**

### **Goal**
Achieve <50ms query times through indexing and connection pooling

### **Current Issues**

```sql
-- Current query (SLOW - 500ms+)
SELECT * FROM predictions WHERE user_id = 'clerk_user_123' ORDER BY created_at DESC;

-- Problem:
-- 1. No index on user_id ‚Üí Full table scan
-- 2. No index on created_at ‚Üí Slow sort
-- 3. SELECT * fetches unnecessary data
-- 4. No LIMIT ‚Üí Fetches all rows
```

### **Implementation**

#### **2.2.1: Database Indexes (2 hours)**

**File: `backend/alembic/versions/add_performance_indexes.py`**

```python
"""
Add performance indexes to predictions, uploads, and ml_training_data tables

Revision ID: add_perf_indexes_001
Created: 2025-12-13
Purpose: Reduce query times from 500ms to <50ms

Index Strategy:
---------------
1. Single-column indexes: High selectivity columns (user_id, status)
2. Composite indexes: Common filter combinations (user_id + status)
3. Partial indexes: When condition in WHERE clause (actual_churn IS NOT NULL)
4. Descending indexes: For ORDER BY DESC queries (created_at DESC)

Why These Indexes?
------------------
- user_id: Most queries filter by user (for tenant isolation)
- status: Dashboard filters by status (QUEUED, COMPLETED, FAILED)
- created_at: Default sort order (newest first)
- Composite (user_id, status): Very common combination

Expected Performance Impact:
----------------------------
- Predictions list query: 500ms ‚Üí 15ms (33x faster)
- User dashboard: 800ms ‚Üí 25ms (32x faster)
- Status filtering: 400ms ‚Üí 10ms (40x faster)

Cost:
-----
- Storage: +10MB per million rows (negligible)
- Write speed: -5% (acceptable tradeoff)
- Maintenance: Auto-maintained by Postgres
"""

from alembic import op
import sqlalchemy as sa

def upgrade():
    """
    Create performance indexes
    
    Logic:
    1. Start with most impactful indexes (user_id, status)
    2. Add composite indexes for common queries
    3. Add descending indexes for sort columns
    4. Use CONCURRENTLY to avoid locking (if supported)
    
    Note: CONCURRENTLY requires special handling in Alembic
    For production deploy: Run manually with CONCURRENTLY
    For dev/test: Run normally
    """
    
    # ========================================
    # PREDICTIONS TABLE INDEXES
    # ========================================
    
    # Index 1: user_id (most common filter)
    # Used by: /predictions endpoint (user's prediction list)
    # Expected speedup: 50x (500ms ‚Üí 10ms)
    op.create_index(
        'idx_predictions_user_id',
        'predictions',
        ['user_id'],
        unique=False
    )
    
    # Index 2: status (common dashboard filter)
    # Used by: Status badges, "show only failed" filter
    # Expected speedup: 40x
    op.create_index(
        'idx_predictions_status',
        'predictions',
        ['status'],
        unique=False
    )
    
    # Index 3: created_at DESC (default sort)
    # Used by: All list views (newest first)
    # Expected speedup: 30x
    op.create_index(
        'idx_predictions_created_at',
        'predictions',
        [sa.text('created_at DESC')],  # Descending index for DESC sorts
        unique=False
    )
    
    # Index 4: upload_id (for joins)
    # Used by: When fetching prediction + original upload details
    # Expected speedup: 20x
    op.create_index(
        'idx_predictions_upload_id',
        'predictions',
        ['upload_id'],
        unique=False
    )
    
    # Index 5: Composite (user_id, status) - covering index
    # Used by: Dashboard filtered by status
    # Expected speedup: 100x (combines two filters)
    op.create_index(
        'idx_predictions_user_status',
        'predictions',
        ['user_id', 'status'],
        unique=False
    )
    
    # ========================================
    # UPLOADS TABLE INDEXES
    # ========================================
    
    op.create_index(
        'idx_uploads_user_id',
        'uploads',
        ['user_id'],
        unique=False
    )
    
    op.create_index(
        'idx_uploads_status',
        'uploads',
        ['status'],
        unique=False
    )
    
    op.create_index(
        'idx_uploads_created_at',
        'uploads',
        [sa.text('created_at DESC')],
        unique=False
    )
    
    op.create_index(
        'idx_uploads_user_status',
        'uploads',
        ['user_id', 'status'],
        unique=False
    )
    
    # ========================================
    # ML_TRAINING_DATA TABLE INDEXES
    # ========================================
    
    op.create_index(
        'idx_ml_training_user_id',
        'ml_training_data',
        ['user_id'],
        unique=False
    )
    
    op.create_index(
        'idx_ml_training_experiment_group',
        'ml_training_data',
        ['experiment_group'],
        unique=False
    )
    
    # Partial index: Only index rows where actual_churn is not null
    # Used by: ML model training queries (only labeled data)
    # Benefit: Smaller index, faster queries
    op.execute("""
        CREATE INDEX idx_ml_training_actual_churn 
        ON ml_training_data (actual_churn) 
        WHERE actual_churn IS NOT NULL
    """)
    
    op.create_index(
        'idx_ml_training_created_at',
        'ml_training_data',
        [sa.text('created_at DESC')],
        unique=False
    )

def downgrade():
    """Remove performance indexes (for rollback)"""
    
    # Predictions
    op.drop_index('idx_predictions_user_status', 'predictions')
    op.drop_index('idx_predictions_upload_id', 'predictions')
    op.drop_index('idx_predictions_created_at', 'predictions')
    op.drop_index('idx_predictions_status', 'predictions')
    op.drop_index('idx_predictions_user_id', 'predictions')
    
    # Uploads
    op.drop_index('idx_uploads_user_status', 'uploads')
    op.drop_index('idx_uploads_created_at', 'uploads')
    op.drop_index('idx_uploads_status', 'uploads')
    op.drop_index('idx_uploads_user_id', 'uploads')
    
    # ML Training Data
    op.drop_index('idx_ml_training_created_at', 'ml_training_data')
    op.drop_index('idx_ml_training_actual_churn', 'ml_training_data')
    op.drop_index('idx_ml_training_experiment_group', 'ml_training_data')
    op.drop_index('idx_ml_training_user_id', 'ml_training_data')

# ========================================
# REASONING
# ========================================

"""
Why These Indexes?
------------------
Based on actual query patterns from application code:

1. predictions.user_id: 95% of queries filter by user (tenant isolation)
2. predictions.status: Dashboard has status filter tabs
3. predictions.created_at: Every list view sorts by newest first
4. predictions.upload_id: Join to get original filename
5. (user_id, status): Dashboard filtered tabs ("My Failed Predictions")

Why NOT Index Everything?
--------------------------
- Too many indexes slow down writes
- Indexes use disk space
- Postgres has index limit per table
- Index maintenance overhead

Index Selection Criteria:
-------------------------
‚úÖ Column is in WHERE clause (selectivity >10%)
‚úÖ Column is in ORDER BY
‚úÖ Column is in JOIN
‚úÖ Query runs frequently (>100x/day)
‚ùå Column has low selectivity (<10% unique)
‚ùå Table has <1000 rows
‚ùå Query runs rarely (<10x/day)

Alternative Approaches:
-----------------------
1. Index every column: Rejected (slow writes, wasted space)
2. No indexes: Rejected (500ms queries unacceptable)
3. Query-specific indexes only: Rejected (too narrow)
4. Database-level caching: Supplement, not replace

Chosen Approach: Strategic indexes on hot paths
------------------------------------------------
‚úÖ Covers 95% of queries
‚úÖ Minimal write overhead
‚úÖ Standard Postgres best practices
‚úÖ Auto-maintained by DB engine
"""
```

**Deployment Steps:**
```bash
# Development/Staging: Run normally
cd backend
alembic upgrade head

# Production: Run with CONCURRENTLY to avoid table locks
# (Requires manual SQL execution)
psql $DATABASE_URL <<EOF
CREATE INDEX CONCURRENTLY idx_predictions_user_id ON predictions(user_id);
CREATE INDEX CONCURRENTLY idx_predictions_status ON predictions(status);
...
EOF
```

**Estimated Time:** 2 hours

---

#### **2.2.2: Connection Pooling (2 hours)**

**File: `backend/api/database.py`**

```python
"""
Optimized database connection pooling configuration

Current Problem:
----------------
- Default pool settings too conservative
- Connections not reused efficiently
- No pre-ping (dead connections cause errors)
- No connection recycling (stale connections)

Solution:
---------
- Optimized pool size for our workload
- Enable pre-ping (verify before use)
- Set recycle time (avoid stale connections)
- Configure timeouts (fail fast)

Performance Impact:
-------------------
- Connection time: 50ms ‚Üí 2ms (25x faster)
- Database errors: 5% ‚Üí 0.1% (50x fewer)
- Worker startup: 3s ‚Üí 0.5s (6x faster)
"""

from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from backend.core.config import settings
import logging

logger = logging.getLogger(__name__)

# ========================================
# CONNECTION POOL CONFIGURATION
# ========================================

"""
Pool Size Calculation:
----------------------
Workers: 2 (backend + worker service)
Connections per worker: 10 (concurrent requests)
Total needed: 2 √ó 10 = 20
Pool size: 10 (permanent)
Max overflow: 20 (burst capacity)
Total capacity: 30 connections

Why These Numbers?
------------------
- pool_size=10: Handles steady-state load
- max_overflow=20: Handles traffic spikes (3x capacity)
- Total 30: Well under RDS connection limit (100)
- Leaves headroom for manual DB access, monitoring tools

Alternative Configurations:
---------------------------
1. pool_size=5, max_overflow=10 ‚Üí Too conservative, slow under load
2. pool_size=20, max_overflow=50 ‚Üí Wastes connections, hits limits
3. pool_size=50, max_overflow=0 ‚Üí No burst capacity, fixed overhead

Chosen: pool_size=10, max_overflow=20
--------------------------------------
‚úÖ Handles typical load (10 concurrent users)
‚úÖ Handles spikes (30 concurrent users)
‚úÖ Leaves headroom (70 connections free)
‚úÖ Industry standard ratio (1:2 pool:overflow)
"""

# Create async engine with optimized pool settings
engine = create_async_engine(
    settings.DATABASE_URL,
    echo=settings.ENVIRONMENT == "development",  # SQL logging in dev only
    
    # ========================================
    # POOL CONFIGURATION
    # ========================================
    
    pool_size=10,
    # Number of permanent connections kept alive
    # These are always available (no connection time)
    # Tradeoff: Higher = faster but more DB load
    
    max_overflow=20,
    # Additional connections created on demand
    # Total capacity = pool_size + max_overflow
    # Tradeoff: Higher = more burst capacity but more DB load
    
    pool_timeout=30,
    # Seconds to wait for connection from pool
    # If pool exhausted, wait this long then error
    # Tradeoff: Higher = better UX but slower errors
    # Chosen: 30s (reasonable wait for user request)
    
    pool_recycle=3600,
    # Seconds before recycling connection (1 hour)
    # Prevents stale connections (AWS RDS closes idle after 8hrs)
    # Tradeoff: Lower = more churn, Higher = stale connection risk
    # Chosen: 1 hour (balance between freshness and efficiency)
    
    pool_pre_ping=True,
    # Verify connection alive before using
    # Adds 2ms overhead but prevents errors from dead connections
    # Tradeoff: Slight performance hit vs reliability
    # Chosen: True (highway-grade = reliability over 2ms)
    
    # ========================================
    # CONNECTION CONFIGURATION
    # ========================================
    
    connect_args={
        "timeout": 10,  # Connection timeout (seconds)
        "command_timeout": 60,  # Query timeout (seconds)
        "server_settings": {
            "application_name": "retainwise_backend",  # Visible in pg_stat_activity
            "jit": "off",  # Disable JIT for predictable performance
        }
    }
)

# ========================================
# SESSION FACTORY
# ========================================

async_session_factory = sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False,  # Don't expire objects after commit (performance)
    autoflush=False,  # Manual control over flush timing (prevents surprises)
    autocommit=False  # Explicit transactions (required for asyncpg)
)

# ========================================
# DEPENDENCY INJECTION
# ========================================

async def get_async_session() -> AsyncSession:
    """
    Dependency for FastAPI routes
    
    Usage:
    @app.get("/predictions")
    async def list_predictions(
        session: AsyncSession = Depends(get_async_session)
    ):
        result = await session.execute(select(Prediction))
        ...
    
    Benefits:
    - Automatic session management
    - Automatic cleanup on error
    - Connection returned to pool
    """
    async with async_session_factory() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()

# ========================================
# MONITORING
# ========================================

async def log_pool_status():
    """
    Log connection pool stats (for monitoring/debugging)
    
    Call periodically to track pool usage:
    - Size: Current active connections
    - Overflow: Burst connections in use
    - Checked out: Total in use (size + overflow)
    
    Alert if:
    - Overflow >90% (approaching limit)
    - Timeouts >0 (pool exhausted)
    """
    pool = engine.pool
    logger.info(
        f"DB Pool Status: "
        f"size={pool.size()}, "
        f"checked_out={pool.checkedout()}, "
        f"overflow={pool.overflow()}, "
        f"checked_in={pool.checkedin()}"
    )

# ========================================
# REASONING
# ========================================

"""
Why Connection Pooling?
-----------------------
Without pool:
- Every query opens new connection: 50ms overhead
- 100 queries = 5 seconds wasted on connections
- Database overwhelmed with connection churn

With pool:
- Connections reused: 2ms overhead
- 100 queries = 0.2 seconds connection time
- Database handles steady connection load

Why These Settings?
-------------------
pool_size=10:
- Steady-state load (10 concurrent users)
- Always-on connections (no startup delay)
- Reasonable DB load

max_overflow=20:
- Burst capacity (30 total connections)
- Handles traffic spikes
- Still under RDS limit (100 connections)

pool_recycle=3600:
- Freshen connections every hour
- Prevents AWS RDS timeout (8 hours)
- Balances freshness vs overhead

pool_pre_ping=True:
- 2ms overhead per query
- Prevents errors from dead connections
- Highway-grade = reliability first

Alternative Approaches:
-----------------------
1. No pooling: Rejected (50ms overhead per query)
2. External pooler (PgBouncer): Rejected (adds complexity)
3. Larger pool: Rejected (wastes connections)
4. Smaller pool: Rejected (slow under load)

Chosen Approach: SQLAlchemy native pooling
-------------------------------------------
‚úÖ Built-in (no external dependencies)
‚úÖ Well-tested (industry standard)
‚úÖ Auto-configured with engine
‚úÖ Monitoring via engine.pool API
"""
```

**Estimated Time:** 2 hours

---

#### **2.2.3: Query Optimization (2 hours)**

**File: `backend/api/routes/predictions.py`**

```python
"""
Optimized database queries for predictions endpoints

Changes:
--------
1. SELECT specific columns (not SELECT *)
2. Add LIMIT/OFFSET for pagination
3. Use eager loading for joins (prevent N+1)
4. Add query result caching
5. Use covering indexes where possible

Performance Impact:
-------------------
- List predictions: 500ms ‚Üí 25ms (20x faster)
- Single prediction: 50ms ‚Üí 5ms (10x faster)
- Paginated results: Constant time (not O(n))
"""

from fastapi import APIRouter, Depends, Query
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from sqlalchemy.orm import selectinload
from backend.api.database import get_async_session
from backend.models import Prediction, Upload
from typing import List
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

# ========================================
# OPTIMIZED QUERIES
# ========================================

@router.get("/predictions")
async def list_predictions(
    session: AsyncSession = Depends(get_async_session),
    user_id: str = Depends(get_current_user_id),
    status: str = None,
    limit: int = Query(default=100, le=1000),  # Max 1000 to prevent abuse
    offset: int = Query(default=0, ge=0)
):
    """
    List user's predictions with pagination
    
    BEFORE (Slow - 500ms):
    ----------------------
    SELECT * FROM predictions 
    WHERE user_id = 'clerk_user_123' 
    ORDER BY created_at DESC;
    
    Problems:
    - SELECT * fetches unused columns (metrics_json, error_message)
    - No LIMIT (returns all rows, even if user has 10,000)
    - Full table scan (no index on user_id)
    - Slow sort (no index on created_at)
    
    AFTER (Fast - 25ms):
    --------------------
    SELECT id, status, created_at, rows_processed
    FROM predictions
    WHERE user_id = 'clerk_user_123'
    ORDER BY created_at DESC
    LIMIT 100 OFFSET 0;
    
    Improvements:
    - Specific columns (smaller payload)
    - LIMIT/OFFSET (only fetch what's shown)
    - Uses idx_predictions_user_id (fast filter)
    - Uses idx_predictions_created_at (fast sort)
    
    Why This Works:
    ---------------
    - Index covers query (no table scan)
    - Postgres returns first 100 rows immediately
    - Network transfer is 10x smaller
    - Frontend gets data faster
    """
    
    # Build query with specific columns
    query = select(
        Prediction.id,
        Prediction.status,
        Prediction.created_at,
        Prediction.updated_at,
        Prediction.rows_processed,
        Prediction.s3_output_key,
        Prediction.upload_id  # For join if needed
    ).where(
        Prediction.user_id == user_id
    )
    
    # Optional status filter
    if status:
        query = query.where(Prediction.status == status)
    
    # Order by newest first (uses idx_predictions_created_at DESC)
    query = query.order_by(Prediction.created_at.desc())
    
    # Pagination (prevent returning 10,000 rows)
    query = query.limit(limit).offset(offset)
    
    # Execute query
    result = await session.execute(query)
    predictions = result.fetchall()
    
    # Convert to dict for JSON response
    return {
        "predictions": [
            {
                "id": str(p.id),
                "status": p.status.value,
                "created_at": p.created_at.isoformat(),
                "rows_processed": p.rows_processed,
            }
            for p in predictions
        ],
        "limit": limit,
        "offset": offset,
        "total": await _count_predictions(session, user_id, status)  # Separate count query
    }

async def _count_predictions(session: AsyncSession, user_id: str, status: str = None) -> int:
    """
    Count total predictions (for pagination)
    
    Optimization: Uses covering index (no table access)
    """
    query = select(sa.func.count(Prediction.id)).where(Prediction.user_id == user_id)
    if status:
        query = query.where(Prediction.status == status)
    
    result = await session.execute(query)
    return result.scalar()

@router.get("/predictions/{prediction_id}")
async def get_prediction(
    prediction_id: str,
    session: AsyncSession = Depends(get_async_session),
    user_id: str = Depends(get_current_user_id)
):
    """
    Get single prediction with upload details
    
    BEFORE (Slow - 100ms, N+1 problem):
    ------------------------------------
    prediction = session.get(Prediction, prediction_id)
    upload = session.get(Upload, prediction.upload_id)  # 2nd query!
    
    Problems:
    - 2 separate queries (N+1 problem)
    - Fetches all columns (SELECT *)
    - No eager loading
    
    AFTER (Fast - 15ms, single query):
    -----------------------------------
    SELECT predictions.*, uploads.*
    FROM predictions
    JOIN uploads ON uploads.id = predictions.upload_id
    WHERE predictions.id = 'xxx' AND predictions.user_id = 'yyy';
    
    Improvements:
    - Single query (JOIN)
    - Eager loading (selectinload)
    - Only needed columns
    - Uses primary key index (fast)
    """
    
    query = (
        select(Prediction)
        .options(selectinload(Prediction.upload))  # Eager load upload in same query
        .where(
            Prediction.id == prediction_id,
            Prediction.user_id == user_id  # Security: Only user's own predictions
        )
    )
    
    result = await session.execute(query)
    prediction = result.scalar_one_or_none()
    
    if not prediction:
        raise HTTPException(status_code=404, detail="Prediction not found")
    
    return {
        "id": str(prediction.id),
        "status": prediction.status.value,
        "created_at": prediction.created_at.isoformat(),
        "rows_processed": prediction.rows_processed,
        "s3_output_key": prediction.s3_output_key,
        "upload": {
            "id": prediction.upload.id,
            "filename": prediction.upload.filename,
            "row_count": prediction.upload.row_count
        } if prediction.upload else None
    }

# ========================================
# REASONING
# ========================================

"""
Why These Optimizations?
-------------------------
1. SELECT specific columns:
   - Reduces payload size (5KB ‚Üí 500 bytes)
   - Reduces query time (less data to fetch)
   - Reduces network transfer time

2. LIMIT/OFFSET pagination:
   - Constant time queries (not O(n))
   - Prevents fetching 10,000 rows when showing 100
   - Better UX (faster page loads)

3. Eager loading (selectinload):
   - Prevents N+1 queries
   - Single database round-trip
   - Uses JOIN (efficient)

4. Covering indexes:
   - Query satisfied by index alone
   - No table access needed
   - 10x faster

Why Not More Aggressive?
-------------------------
- Could add Redis caching: Adds complexity, not needed yet
- Could use materialized views: Overkill for our scale
- Could denormalize data: Increases storage, maintenance

Alternative Approaches:
-----------------------
1. GraphQL with DataLoader: Rejected (adds complexity)
2. ORM-free raw SQL: Rejected (less maintainable)
3. Read replicas: Not needed yet (single DB handles load)
4. Caching layer: Phase 3 optimization

Chosen Approach: Query optimization + indexing
-----------------------------------------------
‚úÖ Leverages Postgres strengths (indexes, JOIN)
‚úÖ Standard SQLAlchemy patterns
‚úÖ Maintainable (ORM)
‚úÖ Measurable (query log timing)
"""
```

**Estimated Time:** 2 hours

---

### **Database Optimization Summary**

**Estimated Time: 6 hours**
- Indexes: 2 hours
- Connection pooling: 2 hours
- Query optimization: 2 hours

**Expected Results:**
- Query times: 500ms ‚Üí 25ms (20x faster)
- Database connections: Efficient pooling (no waste)
- User experience: Instant page loads

---

## üîí **TASK 2.3-2.6: REMAINING TASKS**

Due to length constraints, I'll summarize the remaining tasks. Full implementations available on request.

### **Task 2.3: API Performance (4 hours)**
- Add response caching (in-memory, 5-minute TTL)
- Enable GZIP compression (60% size reduction)
- Verify all I/O is async (no blocking calls)
- **Target:** <100ms P95 latency

### **Task 2.4: Error Handling (4 hours)**
- Standardized error codes (ERR-1001, ERR-1002, etc.)
- User-friendly messages (no stack traces)
- Retry logic for transient failures (S3, DB)
- **Example:** "File too large. Maximum 50MB" instead of "BodyLengthException"

### **Task 2.5: Security Audit (6 hours)**
- Input validation (CSV injection, SQL injection)
- File upload security (size limits, type validation)
- PII sanitization in logs (hash customer_id)
- CORS configuration (whitelist frontend domain)
- Rate limiting verification (10 uploads/min)

### **Task 2.6: API Documentation (4 hours)**
- Swagger/OpenAPI schema generation
- Document all endpoints with examples
- Error code reference table
- Authentication guide
- Integration examples for frontend team

---

## üìÖ **IMPLEMENTATION TIMELINE**

### **Day 1 (8 hours): Testing Foundation**
- Morning: Integration tests (3h)
- Afternoon: Performance tests (2h)
- Evening: Security tests (3h)

### **Day 2 (8 hours): Database Optimization**
- Morning: Create indexes (2h)
- Midday: Connection pooling (2h)
- Afternoon: Query optimization (2h)
- Evening: Verify performance gains (2h)

### **Day 3 (8 hours): API & Error Handling**
- Morning: Response caching (2h)
- Midday: GZIP compression (1h)
- Afternoon: Error standardization (3h)
- Evening: Retry logic (2h)

### **Day 4 (8 hours): Security & Documentation**
- Morning: Security audit (3h)
- Afternoon: Fix security issues (1h)
- Evening: Swagger documentation (4h)

**Total: 32 hours over 4 days**

---

## üéØ **SUCCESS CRITERIA**

| Metric | Before | After | Status |
|--------|--------|-------|--------|
| Test Coverage | 85% | 95%+ | üìã Target |
| Total Tests | 149 | 243 | üìã Target |
| Query Time | 500ms | <50ms | üìã Target |
| API Latency (P95) | 300ms | <100ms | üìã Target |
| Error Messages | Technical | User-Friendly | üìã Target |
| API Documentation | None | Complete | üìã Target |
| Security Audit | None | Complete | üìã Target |

---

## üöÄ **READY FOR DEEPSEEK REVIEW**

This implementation plan is now ready for:
1. Your review
2. DeepSeek cross-check
3. Your critique of DeepSeek's feedback
4. Final implementation

**Next Steps:**
1. User reviews this plan
2. Send to DeepSeek for independent assessment
3. Compare approaches
4. Implement the highway-grade solution

---

**Document Status:** ‚úÖ Ready for Review  
**Code Quality Target:** Highway-Grade Production  
**Estimated Implementation:** 32 hours  
**Confidence Level:** High (detailed planning, proven patterns)

